# ðŸ“ˆ Customer Churn Prediction & Analysis App

## Overview
This repository contains an end-to-end Machine Learning application for predicting customer churn at a telecommunications company. It includes a Jupyter Notebook for data exploration and model training, and a Streamlit web application for real-time and batch predictions, visual analytics, and customer segmentation.

---

## Table of Contents
- [Features](#features)
- [Project Structure](#project-structure)
- [Technologies](#technologies)
- [Installation](#installation)
- [Usage](#usage)
  - [Single Prediction](#single-prediction)
  - [Batch Prediction](#batch-prediction)
  - [Customer Segmentation](#customer-segmentation)
- [Model Workflow](#model-workflow)
- [Model Performance (Summary)](#model-performance-summary)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [Author](#author)

---

## Features
- Real-time single-customer churn prediction via a web form.
- Batch prediction: upload a CSV/XLSX file and receive predictions for many customers.
- Interactive dashboard with visualizations (churn distribution, correlation heatmap, tenure vs. charges).
- Customer segmentation using K-Means clustering to label customers as Loyal, At-Risk, or Moderate Risk.
- Actionable suggestions based on model output (e.g., targeted discounts for at-risk customers).

---

## Project Structure
```
â”œâ”€â”€ Churn_nb.ipynb       # Jupyter Notebook for EDA, preprocessing, model training, evaluation
â”œâ”€â”€ main.py              # Primary Streamlit application (predictions + clustering + dashboard)
â”œâ”€â”€ app.py               # Alternative Streamlit app version (predictions + analysis)
â”œâ”€â”€ home.py              # Home page / component for modular app structure
â”œâ”€â”€ mainn.py             # Multi-page navigation entry point
â”œâ”€â”€ best_model.pkl       # Trained Decision Tree Classifier (generated by notebook)
â”œâ”€â”€ scaler.pkl           # Trained StandardScaler (generated by notebook)
â””â”€â”€ README.md            # Project documentation
```

---

## Technologies
- Python
- Streamlit
- scikit-learn
- pandas, numpy
- matplotlib, seaborn
- joblib (for saving/loading models)
- openpyxl (for Excel file support)

---

## Installation

1. Clone the repository:
```bash
git clone https://github.com/Abdul770562/Churn-Rate-Prediction.git
cd Churn-Rate-Prediction
```

2. (Recommended) Create and activate a virtual environment:
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS / Linux
source venv/bin/activate
```

3. Install dependencies:
If a `requirements.txt` is present:
```bash
pip install -r requirements.txt
```
Or install the common dependencies:
```bash
pip install streamlit pandas numpy scikit-learn matplotlib seaborn joblib openpyxl
```

---

## Usage

### Generate the model (if required)
If `best_model.pkl` and `scaler.pkl` are not included, open `Churn_nb.ipynb`, run all cells to preprocess the data, train the model, and save the trained artifacts (`best_model.pkl` and `scaler.pkl`).

### Run the Streamlit app
Start the main interactive app:
```bash
streamlit run main.py
```
or use other entry points as provided:
```bash
streamlit run app.py
# or
streamlit run mainn.py
```

---

## How to Use the App

### Single Prediction
1. Go to the Single Prediction section of the app.
2. Enter customer details such as:
   - Age
   - Gender
   - Tenure (months with company)
   - Monthly Charges
   - Contract Type (Month-to-month / One year / Two year)
   - Internet Service (Fiber optic / DSL / No)
   - Tech Support (Yes / No)
3. Click "Predict" to see whether the customer is likely to churn.

### Batch Prediction
1. Go to the Batch Prediction section.
2. Upload a .csv or .xlsx file containing customer records (columns should match the features expected by the preprocessing pipeline).
3. The app will process the file, append a `Churn_Prediction` column, and provide a download link for the results.

### Customer Segmentation
1. Upload your predicted dataset (or the processed dataset).
2. The app runs K-Means clustering and visualizes clusters:
   - Cluster 1 (ðŸŸ¦): Loyal customers â€” typically high tenure, stable charges
   - Cluster 2 (ðŸ”´): At-risk customers â€” typically low tenure, higher charges
   - Cluster 3 (ðŸŸ¢): Moderate risk

---

## Model Workflow
1. Data Loading: Load raw customer data (Customer ID, Gender, Tenure, Charges, Contract, etc.).
2. Preprocessing:
   - Encode categorical variables (e.g., Gender, ContractType, InternetService, TechSupport).
   - Handle missing values and convert numeric fields as required.
   - Scale numeric features using `StandardScaler`.
3. Model Selection: Several algorithms were evaluated:
   - Logistic Regression
   - K-Nearest Neighbors
   - Support Vector Classifier (SVC)
   - Decision Tree Classifier (selected)
4. Hyperparameter Tuning: GridSearchCV used to find the best Decision Tree settings.
5. Persist model and scaler as `.pkl` files for deployment.

---

## Model Performance (summary)
- Logistic Regression: ~89.5% accuracy
- K-Nearest Neighbors: ~94% accuracy
- Support Vector Classifier (SVC): ~96% accuracy
- Decision Tree Classifier: ~100% accuracy on the test set (selected model)

Note: A reported perfect or near-perfect score may indicate overfitting. Validate on holdout / cross-validation sets and consider more robust models (Random Forest / XGBoost) for better generalization.

---

## Future Improvements
- Train and evaluate Random Forest and XGBoost models for stronger generalization.
- Add automated model evaluation and retraining pipeline.
- Add user authentication for the web app.
- Integrate automated email alerts for "At-Risk" customers and retention campaigns.
- Add more robust input validation and clearer expected CSV/XLSX schema docs.

---

## Contributing
Contributions are welcome! Please open an issue or submit a pull request with proposed changes. Consider:
- Improving preprocessing and feature engineering
- Adding unit tests
- Expanding the Streamlit UI and documentation

---

## Author
Created by: Mohammed Abdul Qadir Shaikh

---

## License
Specify a license for your project (e.g., MIT). Add a LICENSE file to the repository if you want to make the project open-source.
